{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 15:44:09.574550: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-06 15:44:09.681062: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-06 15:44:09.683752: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/don/anaconda3/envs/cab420/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-06 15:44:09.683766: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-06 15:44:10.133708: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/don/anaconda3/envs/cab420/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-06 15:44:10.133758: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/don/anaconda3/envs/cab420/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-06 15:44:10.133763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=23\n",
    "image_height=800\n",
    "image_width=1200\n",
    "\n",
    "def read_image(x):\n",
    "    x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (image_width, image_height))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "\n",
    "def read_mask(x):\n",
    "    x = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (image_width, image_height))\n",
    "    x = x.astype(np.int32)\n",
    "    return x\n",
    "\n",
    "\n",
    "def tf_dataset(x,y, batch=4):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x,y)) # Dataset object from Tensorflow\n",
    "    dataset = dataset.shuffle(buffer_size=100) \n",
    "    dataset = dataset.map(preprocess) # Applying preprocessing to every batch in the Dataset object\n",
    "    dataset = dataset.batch(batch) # Determine atch-size\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(2) # Optimization\n",
    "    return dataset\n",
    "        \n",
    "\n",
    "def preprocess(x,y):\n",
    "    def f(x,y):\n",
    "        x = x.decode()\n",
    "        y = y.decode()\n",
    "        image = read_image(x)\n",
    "        mask = read_mask(y)\n",
    "        return image, mask\n",
    "    \n",
    "    image, mask = tf.numpy_function(f,[x,y],[tf.float32, tf.int32])\n",
    "    mask = tf.one_hot(mask, num_classes, dtype=tf.int32)\n",
    "    image.set_shape([image_height, image_width, 3])    # In the Images, number of channels = 3. \n",
    "    mask.set_shape([image_height, image_width, num_classes])    # In the Masks, number of channels = number of classes. \n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/don/Git/aerial-semantic-segmentation/dataset_here/dataset/semantic_drone_dataset'\n",
    "img_path = root_dir + '/original_images/'\n",
    "mask_path = root_dir + '/label_images_semantic/'\n",
    "\n",
    "names = list(map(lambda x: x.replace('.jpg', ''), os.listdir(img_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size : 288 images\n",
      "Val Size   :  72 images\n",
      "Test Size  :  40 images\n"
     ]
    }
   ],
   "source": [
    "X_trainval, X_test = train_test_split(names, test_size=0.1, random_state=19)\n",
    "X_train, X_val = train_test_split(X_trainval, test_size=0.2, random_state=19)\n",
    "\n",
    "print(f\"Train Size : {len(X_train)} images\")\n",
    "print(f\"Val Size   :  {len(X_val)} images\")\n",
    "print(f\"Test Size  :  {len(X_test)} images\")\n",
    "\n",
    "y_train = X_train\n",
    "y_test = X_test\n",
    "y_val = X_val\n",
    "\n",
    "img_train = [os.path.join(img_path, f\"{name}.jpg\") for name in X_train]\n",
    "mask_train = [os.path.join(mask_path, f\"{name}.png\") for name in y_train]\n",
    "img_val = [os.path.join(img_path, f\"{name}.jpg\") for name in X_val]\n",
    "mask_val = [os.path.join(mask_path, f\"{name}.png\") for name in y_val]\n",
    "img_test = [os.path.join(img_path, f\"{name}.jpg\") for name in X_test]\n",
    "mask_test = [os.path.join(mask_path, f\"{name}.png\") for name in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 77. 121. 568. 223.  31. 497.   3.   5. 320. 433.  55. 260. 141.  38.\n 216. 175. 136. 451. 208.  43. 323. 582. 335. 122. 138. 304. 325. 311.\n 165. 480. 104. 312. 383. 408. 301. 110. 475. 177. 455. 228.  58. 163.\n 180. 431. 148. 314. 273. 412. 302. 390. 188. 234. 592. 419. 161.  57.\n 538.  89. 576. 318. 199. 590. 341. 128.  41. 288. 113. 554. 159. 443.\n 294. 154.  18. 515. 217. 119.  52. 580. 101. 345.  16.  99. 149. 445.\n 591. 501. 292. 134. 330. 222. 545. 265. 173. 381.   6. 130. 414. 193.\n 263. 598. 289. 107. 543. 213. 225. 190. 351. 146. 276. 170. 452. 251.\n 349. 570. 535. 298. 200. 410. 464. 434. 421. 457. 157. 321. 305. 435.\n 232.  44. 126.  62.   1. 147.  88. 139. 425. 382. 484. 584. 426. 332.\n 233.  51. 167. 389. 156. 427. 366. 587. 313. 235. 220. 521.  78. 494.\n 380. 361. 192. 176. 498. 424. 329. 299. 281. 403.  11. 463. 226.  56.\n 499. 488. 583.  47. 240. 458. 430. 117. 439. 194. 275. 559. 533. 508.\n  71. 493.  19. 524. 537. 296. 135. 572. 204. 518. 406. 476. 326. 342.\n 525. 271. 378. 111.  40. 202.  49. 569.  63. 179. 563. 530. 567.  42.\n 198. 579.  95. 346. 158. 356.  70. 118. 529. 512. 295. 465.  22. 509.\n   0. 566.  14. 372.  98.  35.  75. 287. 112. 385. 106. 460. 398. 551.\n 238. 331. 215. 309. 485. 153. 339. 195. 549. 423. 348.  83. 322. 306.\n 388. 574.  74. 355. 442. 526. 344. 248. 560.   8. 437.  53. 243. 544.\n 178. 517. 474.  26.  13.  81.  79. 531. 585. 181. 145. 239. 470. 454.\n 536. 219. 316. 447. 367. 363. 413. 290.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Initialize and train the SVM classifier\u001b[39;00m\n\u001b[1;32m      5\u001b[0m svm \u001b[39m=\u001b[39m SVC()\n\u001b[0;32m----> 6\u001b[0m svm\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Make predictions on the validation set\u001b[39;00m\n\u001b[1;32m      9\u001b[0m y_val_pred \u001b[39m=\u001b[39m svm\u001b[39m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m~/anaconda3/envs/cab420/lib/python3.10/site-packages/sklearn/svm/_base.py:192\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    190\u001b[0m     check_consistent_length(X, y)\n\u001b[1;32m    191\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    193\u001b[0m         X,\n\u001b[1;32m    194\u001b[0m         y,\n\u001b[1;32m    195\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[1;32m    196\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    197\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    198\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    199\u001b[0m     )\n\u001b[1;32m    201\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_targets(y)\n\u001b[1;32m    203\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[1;32m    204\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[1;32m    205\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/cab420/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    564\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/cab420/lib/python3.10/site-packages/sklearn/utils/validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[0;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/anaconda3/envs/cab420/lib/python3.10/site-packages/sklearn/utils/validation.py:902\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    903\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    905\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    906\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    907\u001b[0m         )\n\u001b[1;32m    909\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    910\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    911\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    913\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 77. 121. 568. 223.  31. 497.   3.   5. 320. 433.  55. 260. 141.  38.\n 216. 175. 136. 451. 208.  43. 323. 582. 335. 122. 138. 304. 325. 311.\n 165. 480. 104. 312. 383. 408. 301. 110. 475. 177. 455. 228.  58. 163.\n 180. 431. 148. 314. 273. 412. 302. 390. 188. 234. 592. 419. 161.  57.\n 538.  89. 576. 318. 199. 590. 341. 128.  41. 288. 113. 554. 159. 443.\n 294. 154.  18. 515. 217. 119.  52. 580. 101. 345.  16.  99. 149. 445.\n 591. 501. 292. 134. 330. 222. 545. 265. 173. 381.   6. 130. 414. 193.\n 263. 598. 289. 107. 543. 213. 225. 190. 351. 146. 276. 170. 452. 251.\n 349. 570. 535. 298. 200. 410. 464. 434. 421. 457. 157. 321. 305. 435.\n 232.  44. 126.  62.   1. 147.  88. 139. 425. 382. 484. 584. 426. 332.\n 233.  51. 167. 389. 156. 427. 366. 587. 313. 235. 220. 521.  78. 494.\n 380. 361. 192. 176. 498. 424. 329. 299. 281. 403.  11. 463. 226.  56.\n 499. 488. 583.  47. 240. 458. 430. 117. 439. 194. 275. 559. 533. 508.\n  71. 493.  19. 524. 537. 296. 135. 572. 204. 518. 406. 476. 326. 342.\n 525. 271. 378. 111.  40. 202.  49. 569.  63. 179. 563. 530. 567.  42.\n 198. 579.  95. 346. 158. 356.  70. 118. 529. 512. 295. 465.  22. 509.\n   0. 566.  14. 372.  98.  35.  75. 287. 112. 385. 106. 460. 398. 551.\n 238. 331. 215. 309. 485. 153. 339. 195. 549. 423. 348.  83. 322. 306.\n 388. 574.  74. 355. 442. 526. 344. 248. 560.   8. 437.  53. 243. 544.\n 178. 517. 474.  26.  13.  81.  79. 531. 585. 181. 145. 239. 470. 454.\n 536. 219. 316. 447. 367. 363. 413. 290.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize and train the SVM classifier\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = svm.predict(X_val)\n",
    "\n",
    "# Calculate the accuracy of the SVM on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# Print the validation accuracy\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cab420",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45b47186e6dc02e8188f53c9d4fddcabd6ad86b62bbaffc23eab8b6b15b0a547"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
